{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install gensim\n",
    "!pip install conllu\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'PerDT/fa_perdt-ud-dev.conllu'\n",
    "data_file = open(path, \"r\", encoding=\"utf-8\")\n",
    "generator = parse_incr(data_file)\n",
    "\n",
    "sentence_models = []\n",
    "\n",
    "for sentence_model in generator: \n",
    "    sentence_models.append(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sentences = []\n",
    "\n",
    "model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "for sentence_model in sentence_models:\n",
    "    text = sentence_model.metadata['text']\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    bert_sentences.append(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert.txt', 'w', encoding='utf-16') as f:\n",
    "    for sent in bert_sentences:\n",
    "        line = \" \".join(sent)\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['به گزارش خبرنگار مهر در گرگان ، بر اساس باورهای دینی ترکمن [ZWNJ] ها در این روز برای پیامبر اکرم ( ص ) ناراحتی و بیماری رخ داد که چند روز بعد با رح ##لت نبی مکر ##م اسلام جهان عزادار مات ##مش شد .']\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "with open('bert.txt', 'r', encoding='utf-16') as f:\n",
    "    lines.append(f.readline().strip())\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['به',\n",
       "  'گزارش',\n",
       "  'خبرنگار',\n",
       "  'مهر',\n",
       "  'در',\n",
       "  'گرگان',\n",
       "  '،',\n",
       "  'بر',\n",
       "  'اساس',\n",
       "  'باورهای',\n",
       "  'دینی',\n",
       "  'ترکمن',\n",
       "  '[ZWNJ]',\n",
       "  'ها',\n",
       "  'در',\n",
       "  'این',\n",
       "  'روز',\n",
       "  'برای',\n",
       "  'پیامبر',\n",
       "  'اکرم',\n",
       "  '(',\n",
       "  'ص',\n",
       "  ')',\n",
       "  'ناراحتی',\n",
       "  'و',\n",
       "  'بیماری',\n",
       "  'رخ',\n",
       "  'داد',\n",
       "  'که',\n",
       "  'چند',\n",
       "  'روز',\n",
       "  'بعد',\n",
       "  'با',\n",
       "  'رح',\n",
       "  '##لت',\n",
       "  'نبی',\n",
       "  'مکر',\n",
       "  '##م',\n",
       "  'اسلام',\n",
       "  'جهان',\n",
       "  'عزادار',\n",
       "  'مات',\n",
       "  '##مش',\n",
       "  'شد',\n",
       "  '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_sentences = []\n",
    "for line in lines:\n",
    "    b_sentences.append(line.split())\n",
    "\n",
    "b_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for sentence_model in sentence_models:\n",
    "    words = [token['lemma'] for token in sentence_model]\n",
    "    sentences.append(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.044631</td>\n",
       "      <td>-0.019841</td>\n",
       "      <td>-0.023262</td>\n",
       "      <td>0.074152</td>\n",
       "      <td>-0.149207</td>\n",
       "      <td>-0.132720</td>\n",
       "      <td>0.447928</td>\n",
       "      <td>0.181325</td>\n",
       "      <td>-0.333659</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251829</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>-0.053167</td>\n",
       "      <td>-0.054675</td>\n",
       "      <td>0.497390</td>\n",
       "      <td>0.198481</td>\n",
       "      <td>-0.006749</td>\n",
       "      <td>-0.259465</td>\n",
       "      <td>0.092721</td>\n",
       "      <td>0.085204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>و</th>\n",
       "      <td>0.018250</td>\n",
       "      <td>-0.028154</td>\n",
       "      <td>-0.037810</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>-0.108691</td>\n",
       "      <td>-0.117980</td>\n",
       "      <td>0.455847</td>\n",
       "      <td>0.160933</td>\n",
       "      <td>-0.362743</td>\n",
       "      <td>0.070451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315893</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>-0.032276</td>\n",
       "      <td>-0.043860</td>\n",
       "      <td>0.536335</td>\n",
       "      <td>0.220656</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>-0.325333</td>\n",
       "      <td>0.094050</td>\n",
       "      <td>0.081780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>کرد#کن</th>\n",
       "      <td>0.007487</td>\n",
       "      <td>-0.031638</td>\n",
       "      <td>-0.018479</td>\n",
       "      <td>0.046961</td>\n",
       "      <td>-0.094246</td>\n",
       "      <td>-0.123060</td>\n",
       "      <td>0.421579</td>\n",
       "      <td>0.180790</td>\n",
       "      <td>-0.363657</td>\n",
       "      <td>0.016940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344900</td>\n",
       "      <td>-0.016122</td>\n",
       "      <td>-0.059570</td>\n",
       "      <td>-0.031726</td>\n",
       "      <td>0.494030</td>\n",
       "      <td>0.209568</td>\n",
       "      <td>0.051093</td>\n",
       "      <td>-0.309310</td>\n",
       "      <td>0.099283</td>\n",
       "      <td>0.063402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>به</th>\n",
       "      <td>0.057874</td>\n",
       "      <td>-0.060395</td>\n",
       "      <td>-0.024552</td>\n",
       "      <td>0.047123</td>\n",
       "      <td>-0.130799</td>\n",
       "      <td>-0.083250</td>\n",
       "      <td>0.474455</td>\n",
       "      <td>0.192794</td>\n",
       "      <td>-0.355014</td>\n",
       "      <td>0.074864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325420</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>-0.032906</td>\n",
       "      <td>-0.064811</td>\n",
       "      <td>0.500713</td>\n",
       "      <td>0.233540</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>-0.327781</td>\n",
       "      <td>0.064142</td>\n",
       "      <td>0.068599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>را</th>\n",
       "      <td>0.027470</td>\n",
       "      <td>-0.027160</td>\n",
       "      <td>-0.038821</td>\n",
       "      <td>0.037965</td>\n",
       "      <td>-0.110586</td>\n",
       "      <td>-0.089733</td>\n",
       "      <td>0.433595</td>\n",
       "      <td>0.172429</td>\n",
       "      <td>-0.372748</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308175</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>-0.035309</td>\n",
       "      <td>-0.031173</td>\n",
       "      <td>0.511251</td>\n",
       "      <td>0.219476</td>\n",
       "      <td>0.029181</td>\n",
       "      <td>-0.314938</td>\n",
       "      <td>0.080820</td>\n",
       "      <td>0.045283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       ".       0.044631 -0.019841 -0.023262  0.074152 -0.149207 -0.132720  0.447928   \n",
       "و       0.018250 -0.028154 -0.037810  0.056468 -0.108691 -0.117980  0.455847   \n",
       "کرد#کن  0.007487 -0.031638 -0.018479  0.046961 -0.094246 -0.123060  0.421579   \n",
       "به      0.057874 -0.060395 -0.024552  0.047123 -0.130799 -0.083250  0.474455   \n",
       "را      0.027470 -0.027160 -0.038821  0.037965 -0.110586 -0.089733  0.433595   \n",
       "\n",
       "              7         8         9   ...        40        41        42  \\\n",
       ".       0.181325 -0.333659  0.033825  ...  0.251829  0.019118 -0.053167   \n",
       "و       0.160933 -0.362743  0.070451  ...  0.315893  0.006853 -0.032276   \n",
       "کرد#کن  0.180790 -0.363657  0.016940  ...  0.344900 -0.016122 -0.059570   \n",
       "به      0.192794 -0.355014  0.074864  ...  0.325420  0.003204 -0.032906   \n",
       "را      0.172429 -0.372748  0.051758  ...  0.308175  0.003813 -0.035309   \n",
       "\n",
       "              43        44        45        46        47        48        49  \n",
       ".      -0.054675  0.497390  0.198481 -0.006749 -0.259465  0.092721  0.085204  \n",
       "و      -0.043860  0.536335  0.220656  0.001298 -0.325333  0.094050  0.081780  \n",
       "کرد#کن -0.031726  0.494030  0.209568  0.051093 -0.309310  0.099283  0.063402  \n",
       "به     -0.064811  0.500713  0.233540  0.044459 -0.327781  0.064142  0.068599  \n",
       "را     -0.031173  0.511251  0.219476  0.029181 -0.314938  0.080820  0.045283  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension = 50\n",
    "embeddings = w2v(sentences=sentences, min_count=3, vector_size=embedding_dimension, window=3, sg=1, workers=4)\n",
    "\n",
    "emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [embeddings.wv.get_vector(str(n)) for n in embeddings.wv.key_to_index],\n",
    "        index = embeddings.wv.key_to_index\n",
    "    )\n",
    ")\n",
    "\n",
    "lemma_to_index = embeddings.wv.key_to_index\n",
    "\n",
    "print(emb_df.shape)\n",
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "per_dt_average_sentence_length = 17\n",
    "\n",
    "xpos_to_index_dictionary = {\n",
    "    None          : -1,\n",
    "    '_'           : -1, \n",
    "    'ADJ'         : 0,\n",
    "    'ADP'         : 1,\n",
    "    'ADV'         : 2,\n",
    "    'AUX'         : 3,\n",
    "    'CCONJ'       : 4,\n",
    "    'DET'         : 5,\n",
    "    'INTJ'        : 6,\n",
    "    'NOUN'        : 7,\n",
    "    'NUM'         : 8,\n",
    "    'PART'        : 9,\n",
    "    'PRON'        : 10,\n",
    "    'PROPN'       : 11,\n",
    "    'PUNCT'       : 12,\n",
    "    'SCONJ'       : 13,\n",
    "    'VERB'        : 14,\n",
    "    'X'           : 15,\n",
    "    'PREP'        : 16,\n",
    "    'N_IANM'      : 17,\n",
    "    'N_ANM'       : 18,\n",
    "    'PUNC'        : 19,\n",
    "    'ADJ_AJP'     : 20,\n",
    "    'PREM_DEMAJ'  : 21,\n",
    "    'PR_SEPER'    : 22,\n",
    "    'V_PASS'      : 23,\n",
    "    'CONJ'        : 24,\n",
    "    'PSUS'        : 25,\n",
    "    'ADV_SADV'    : 26,\n",
    "    'PRENUM'      : 27,\n",
    "    'POSTP'       : 28,\n",
    "    'V_ACT'       : 29,\n",
    "    'PR_DEMON'    : 30,\n",
    "    'SUBR'        : 31,\n",
    "    'V_MODL'      : 32,\n",
    "    'IDEN'        : 33,\n",
    "    'POSNUM'      : 34,\n",
    "    'PREM_AMBAJ'  : 35,\n",
    "    'PR_CREFX'    : 36,\n",
    "    'PRENUM_IANM' : 37,\n",
    "    'PR_JOPER'    : 38,\n",
    "    'ADJ_AJCM'    : 39,\n",
    "    'ADJ_AJSUP'   : 40,\n",
    "    'PR_INTG'     : 41,\n",
    "    'PR_UCREFX'   : 42,\n",
    "    'PREM_QUAJ'   : 43,\n",
    "    'PREM_EXAJ'   : 44,\n",
    "    'PR_RECPR'    : 45,\n",
    "    'ADR_PRADR'   : 46,\n",
    "    'AUX_PASS'    : 47 }\n",
    "\n",
    "upos_to_index_dictionary = {\n",
    "    '_'     : -1, \n",
    "    'ADJ'   : 0,\n",
    "    'ADP'   : 1,\n",
    "    'ADV'   : 2,\n",
    "    'AUX'   : 3,\n",
    "    'CCONJ' : 4,\n",
    "    'DET'   : 5,\n",
    "    'INTJ'  : 6,\n",
    "    'NOUN'  : 7,\n",
    "    'NUM'   : 8,\n",
    "    'PART'  : 9,\n",
    "    'PRON'  : 10,\n",
    "    'PROPN' : 11,\n",
    "    'PUNCT' : 12,\n",
    "    'SCONJ' : 13,\n",
    "    'VERB'  : 14,\n",
    "    'X'     : 15,\n",
    "}\n",
    "\n",
    "upos_length = len(upos_to_index_dictionary)\n",
    "xpos_length = len(xpos_to_index_dictionary)\n",
    "pos_length = upos_length + xpos_length\n",
    "\n",
    "lexical_morpho_models = []\n",
    "\n",
    "for sentence_model in sentence_models:\n",
    "    lexical_morpho_model = []\n",
    "    \n",
    "    for i in range(len(sentence_model)):\n",
    "\n",
    "        if i >= per_dt_average_sentence_length:\n",
    "            break\n",
    "\n",
    "        token = sentence_model[i]\n",
    "        lemma = token['lemma']\n",
    "        upos = token['upos']\n",
    "        xpos = token['xpos']\n",
    "\n",
    "        if lemma in lemma_to_index:\n",
    "            upos_index = upos_to_index_dictionary[upos]\n",
    "            xpos_index = xpos_to_index_dictionary[xpos]\n",
    "\n",
    "            upos_vector = [0] * upos_length\n",
    "            xpos_vector = [0] * xpos_length\n",
    "            \n",
    "            if upos_index != -1:\n",
    "                upos_vector[upos_index] = 1\n",
    "\n",
    "            if xpos_index != -1:    \n",
    "                xpos_vector[xpos_index] = 1\n",
    "\n",
    "            vector = xpos_vector + upos_vector\n",
    "            lexical_morpho_model.append(vector)\n",
    "        else:\n",
    "            lexical_morpho_model.append([0] * pos_length)\n",
    "    \n",
    "    lexical_morpho_models.append(lexical_morpho_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_index_dictionary = {\n",
    "    ('Number', 'Sing')   : 0,\n",
    "    ('Number', 'Plur')   : 1,\n",
    "    ('Person', '2')      : 2,\n",
    "    ('Person', '3')      : 3,\n",
    "    ('Person', '1')      : 4,\n",
    "    ('Tense', 'Past')    : 5,\n",
    "    ('Tense', 'Pres')    : 6,\n",
    "    ('Tense', 'Fut')     : 7,\n",
    "    ('Voice', 'Pass')    : 8,\n",
    "    ('Voice', 'Act')     : 9,\n",
    "    ('VerbForm', 'Part') : 10,\n",
    "    ('VerbForm', 'Fin')  : 11,\n",
    "    ('PronType', 'Prs')  : 12,\n",
    "    ('Polarity', 'Neg')  : 13,\n",
    "    ('Mood', 'Imp')      : 14,\n",
    "    ('Mood', 'Sub')      : 15,\n",
    "}\n",
    "\n",
    "feature_length = len(feature_to_index_dictionary)\n",
    "feature_models = []\n",
    "\n",
    "for sentence_model in sentence_models:\n",
    "    feature_model = []\n",
    "    \n",
    "    for i in range(len(sentence_model)):\n",
    "\n",
    "        if i >= per_dt_average_sentence_length:\n",
    "            break\n",
    "\n",
    "        token = sentence_model[i]\n",
    "        lemma = token['lemma']\n",
    "        feats = token['feats']\n",
    "        vector = [0] * feature_length\n",
    "\n",
    "        if feats is None:\n",
    "            continue\n",
    "\n",
    "        for pair in feats.items():\n",
    "            index = feature_to_index_dictionary[pair]\n",
    "            vector[index] = 1\n",
    "    \n",
    "    feature_models.append(feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.txt', 'w', encoding='utf-16') as f:\n",
    "    for key in w.wv.key_to_index:\n",
    "        word = key\n",
    "        vector = w.wv.get_vector(str(key))\n",
    "        text_list = [str(v) for v in vector]\n",
    "        vector_text = \" \".join(text_list)\n",
    "        f.write(word)\n",
    "        f.write(' ')\n",
    "        f.write(vector_text)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b36ac4c20b6eded268908113cfe1618b917998bc3f006721399009fc11fa0862"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
